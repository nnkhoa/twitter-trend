{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import Dataset, Example\n",
    "from torch.utils.data.dataset import random_split\n",
    "# from torchtext.vocab import \n",
    "import os\n",
    "import gensim\n",
    "import pandas as pd\n"
   ]
  },
  {
   "source": [
    "### Load Data with NGrams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "\n",
    "tokenizer = lambda x: x.split()\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, lower=False)\n",
    "NEWS_TYPE = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "fields = {'text': ('t', TEXT), 'type': ('nt', NEWS_TYPE)} \n",
    "\n",
    "df_dataset = pd.read_json('dataset_full.json')\n",
    "\n",
    "df = df_dataset[['text', 'type']]\n",
    "\n",
    "ltoi = {l: i for i, l in enumerate(df['type'].unique())}\n",
    "df['type'] = df['type'].apply(lambda y: ltoi[y])\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, fields: list):\n",
    "        super(DataFrameDataset, self).__init__(\n",
    "            [\n",
    "                Example.fromlist(list(r), fields) for i, r in df.iterrows()\n",
    "            ],\n",
    "            fields\n",
    "            \n",
    "        )\n",
    "dataset = DataFrameDataset(df, fields=(('text', TEXT), ('label', NEWS_TYPE)))\n",
    "\n",
    "train_len = int(len(dataset) * 0.9)\n",
    "valid_len = int(len(dataset) * 0.05)\n",
    "sub_train_, sub_valid_, sub_test_ = random_split(dataset, [train_len, valid_len, len(dataset) - train_len - valid_len])\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "source": [
    "### Train Word2Vec Embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "tweets = [row.split() for row in df['text']]\n",
    "\n",
    "phrases = Phrases(tweets, min_count=2, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "tweets_bigram = bigram[tweets]\n",
    "\n",
    "w2v_model = Word2Vec(min_count=2,\n",
    "                    window=5,\n",
    "                    size=300,\n",
    "                    sample=6e-5,\n",
    "                    alpha=0.03,\n",
    "                    min_alpha=0.0007,\n",
    "                    negative=20)\n",
    "\n",
    "w2v_model.build_vocab(tweets_bigram, progress_per=10000)\n",
    "\n",
    "w2v_model.train(tweets_bigram, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "w2v_model.init_sims(replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "TEXT.build_vocab(dataset, min_freq=2)\n",
    "\n",
    "word2vec_vectors = []\n",
    "for token, idx in TEXT.vocab.stoi.items():\n",
    "    if token in w2v_model.wv.vocab.keys():\n",
    "        word2vec_vectors.append(torch.FloatTensor(w2v_model[token]))\n",
    "    else:\n",
    "        word2vec_vectors.append(torch.zeros(300))\n",
    "\n",
    "TEXT.vocab.set_vectors(TEXT.vocab.stoi, word2vec_vectors, 300)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "embedding = nn.Embedding.from_pretrained(pre_trained_emb)\n"
   ]
  },
  {
   "source": [
    "## The Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, embedding, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.5\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "source": [
    "## Initiate Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "NUM_CLASS = 4\n",
    "\n",
    "model = TextSentiment(embedding, EMBED_DIM, NUM_CLASS).to(device)"
   ]
  },
  {
   "source": [
    "## Generate Batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    labels = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "\n",
    "    return text, offsets, labels"
   ]
  },
  {
   "source": [
    "## Train/Test function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, \n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss/len(sub_train_), train_acc/len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch) \n",
    "\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss/len(data_), acc/len(data_)   "
   ]
  },
  {
   "source": [
    "### Split Dataset and Train the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'sort_key'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d937691ece15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# NEWS_TYPE.build_vocab(dataset, min_freq=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m train_iter, valid_iter, test_iter = BucketIterator.splits(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_valid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_test_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/twitter/lib/python3.8/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, datasets, batch_sizes, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             ret.append(cls(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/twitter/lib/python3.8/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, sort_key, device, batch_size_fn, train, repeat, shuffle, sort, sort_within_batch)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_within_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msort_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'sort_key'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "N_EPOCHS = 10\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "# NEWS_TYPE.build_vocab(dataset, min_freq=2)\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    datasets=(sub_train_, sub_valid_, sub_test_), \n",
    "    batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "    sort_key=None\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_iter)\n",
    "    valid_loss, valid_acc = test(valid_iter)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), ' | time in %d minutes, %d seconds' %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "source": [
    "### Evaluate on Test Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking the results of test dataset...\n\tLoss: 0.0002(test)\t|\tAcc: 89.5%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_iter)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "source": [
    "### Test on random news"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is a Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token] for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "text_str = 'Massive explosions of energy happening thousands of light-years from Earth may have left traces in our planet\\'s biology and geology, according to new research by University of Colorado Boulder geoscientist Robert Brakenridge. The study, published this month in the International Journal of Astrobiology, probes the impacts of supernovas, some of the most violent events in the known universe. In the span of just a few months, a single one of these eruptions can release as much energy as the sun will during its entire lifetime. They\\'re also bright -- really bright. \"We see supernovas in other galaxies all the time,\" said Brakenridge, a senior research associate at the Institute of Arctic and Alpine Research (INSTAAR) at CU Boulder. \"Through a telescope, a galaxy is a little misty spot. Then, all of a sudden, a star appears and may be as bright as the rest of the galaxy.\" A very nearby supernova could be capable of wiping human civilization off the face of the Earth. But even from farther away, these explosions may still take a toll, Brakenridge said, bathing our planet in dangerous radiation and damaging its protective ozone layer. To study those possible impacts, Brakenridge searched through the planet\\'s tree ring records for the fingerprints of these distant, cosmic explosions. His findings suggest that relatively close supernovas could theoretically have triggered at least four disruptions to Earth\\'s climate over the last 40,000 years. The results are far from conclusive, but they offer tantalizing hints that, when it comes to the stability of life on Earth, what happens in space doesn\\'t always stay in space.'\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}