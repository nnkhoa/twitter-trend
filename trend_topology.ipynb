{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('twitter': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9a7792acf25e8797fab3c674d79690d254e723492b4b5eb6b43e888f7969c977"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys \n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import jsonl_parser\n",
    "import text_preprocess\n",
    "import text_features\n",
    "import data_seperation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_raw_data(folder_path, trend_list, lang, verbose):\n",
    "    # all_file = [join(folder_path, f) for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "    # n_sample = 40\n",
    "\n",
    "    list_file = [join(folder_path, f + \".jsonl\") for f in trend_list if isfile(join(folder_path, f +  \".jsonl\"))]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # if n_sample > len(trend_list):\n",
    "    #     n_sample = len(trend_list)\n",
    "\n",
    "    # for f in list_file[:n_sample]:\n",
    "    for f in list_file:\n",
    "        data.extend(jsonl_parser.load_jsonl(f, verbose))\n",
    "\n",
    "    print() \n",
    "\n",
    "    data_lang = [entry for entry in data if entry['lang'] == lang]\n",
    "\n",
    "    return data_lang\n",
    "\n",
    "def print_top_word(model, features_name, num_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" / \".join(features_name[i] for i in topic.argsort()[:-num_words - 1:-1])\n",
    "        print(message)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load annotated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/khoanguyen/Workspace/git/twitter-trend/jsonl_parser.py\", line 12, in load_jsonl\n",
      "    data.append(json.loads(line.rstrip('\\n|\\r')))\n",
      "  File \"/Users/khoanguyen/miniconda3/envs/twitter/lib/python3.8/json/__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/Users/khoanguyen/miniconda3/envs/twitter/lib/python3.8/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/Users/khoanguyen/miniconda3/envs/twitter/lib/python3.8/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 662 (char 661)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trend_type = None\n",
    "\n",
    "annotated = data_seperation.get_data(trend_type)\n",
    "\n",
    "trend_type = annotated[['id', 'type']]\n",
    "trend_hash = annotated['id'].tolist()\n",
    "trend_name = annotated['name'].tolist()\n",
    "trend_dict = dict(zip(trend_hash, trend_name))\n"
   ]
  },
  {
   "source": [
    "### Load Tweet Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_raw_data('/Users/khoanguyen/Workspace/dataset/twitter-trending/TT-classification/dataset-full/', trend_hash, 'en', False)\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "source": [
    "### Mapping Trend from annotated dataframe into Tweet data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trend_name'] = df['trend_hash'].map(trend_dict)\n",
    "\n",
    "df['trend_hash'].value_counts()\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: text_preprocess.remove_hyperlink(x))\n",
    "\n",
    "df_trend = df[['text', 'trend_hash', 'trend_name']]\n",
    "\n",
    "trend_label = text_features.trend_mapping(set(df_trend['trend_hash'].tolist()))\n",
    "\n",
    "df_trend = (df_trend.merge(trend_type, left_on='trend_hash', right_on='id').reindex(columns=['text', 'trend_hash', 'trend_name', 'type']))\n",
    "\n",
    "df_trend['label'] = df_trend.apply(lambda row: trend_label[row.trend_hash], axis=1)"
   ]
  },
  {
   "source": [
    "### Removing trend with less than 10 tweets (if neccessary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend = df_trend.groupby('label')\n",
    "df_trend = df_trend.filter(lambda x: len(x) > 10)"
   ]
  },
  {
   "source": [
    "## Text Features \n",
    "\n",
    "### Unigram "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_uni, features_uni = text_features.generate_term_freq(df['text'], 1)\n",
    "\n",
    "tf_uni_array = tf_uni.toarray()\n"
   ]
  },
  {
   "source": [
    "### Bigram "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_bi, features_bi = text_features.generate_term_freq(df_trend['text'], 2)\n",
    "\n",
    "tf_bi_array = tf_bi.toarray()\n"
   ]
  },
  {
   "source": [
    "### TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, terms = text_features.generate_tfidf(df_trend['text'])\n",
    "\n",
    "tfidf_array = tfidf.toarray()"
   ]
  },
  {
   "source": [
    "### TF-IDF without removing special character and keep true casing - Loose TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(lowercase=False,\n",
    "                                token_pattern=r'\\S+',\n",
    "                                stop_words='english')\n",
    "\n",
    "loose_tfidf = tfidf_vector.fit_transform(df_trend['text'])"
   ]
  },
  {
   "source": [
    "## Tweet Topology Classification\n",
    "\n",
    "### Linear SVM using loose TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               precision    recall  f1-score   support\n\nongoing-event       0.99      0.96      0.98      2524\n         meme       0.99      0.96      0.97     18618\n         news       0.98      0.96      0.97     18757\ncommemorative       0.97      0.99      0.98     44677\n\n     accuracy                           0.98     84576\n    macro avg       0.98      0.97      0.97     84576\n weighted avg       0.98      0.98      0.98     84576\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_name = df_trend['type'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(loose_tfidf, class_name, df_trend.index, test_size=0.25, random_state=0, stratify=class_name)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "new_svc_model = LinearSVC()\n",
    "\n",
    "new_svc_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = new_svc_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=df_trend['type'].unique()))"
   ]
  },
  {
   "source": [
    "### Mutinominal Nayes Bay with loose TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               precision    recall  f1-score   support\n\nongoing-event       1.00      0.38      0.55      2524\n         meme       0.98      0.80      0.88     18618\n         news       0.97      0.81      0.88     18757\ncommemorative       0.84      0.99      0.91     44677\n\n     accuracy                           0.89     84576\n    macro avg       0.95      0.75      0.81     84576\n weighted avg       0.91      0.89      0.89     84576\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "new_MNB_model = MultinomialNB()\n",
    "new_MNB_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = new_MNB_model.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=df_trend['type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}