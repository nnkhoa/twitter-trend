{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('twitter': conda)",
   "display_name": "Python 3.8.5 64-bit ('twitter': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9a7792acf25e8797fab3c674d79690d254e723492b4b5eb6b43e888f7969c977"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys \n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import jsonl_parser\n",
    "import text_preprocess\n",
    "import text_features\n",
    "import data_seperation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "def load_raw_data(folder_path, trend_list, lang):\n",
    "    # all_file = [join(folder_path, f) for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "    n_sample = 40\n",
    "\n",
    "    list_file = [join(folder_path, f + \".jsonl\") for f in trend_list if isfile(join(folder_path, f +  \".jsonl\"))]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    if n_sample > len(trend_list):\n",
    "        n_sample = len(trend_list)\n",
    "\n",
    "    for f in list_file[:n_sample]:\n",
    "        data.extend(jsonl_parser.load_jsonl(f))\n",
    "\n",
    "    print() \n",
    "\n",
    "    data_lang = [entry for entry in data if entry['lang'] == lang]\n",
    "\n",
    "    return data_lang\n",
    "\n",
    "def print_top_word(model, features_name, num_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" / \".join(features_name[i] for i in topic.argsort()[:-num_words - 1:-1])\n",
    "        print(message)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = data_seperation.get_data('news')\n",
    "\n",
    "trend_list = annotated['id'].tolist()\n",
    "\n",
    "data = load_raw_data('/Users/khoanguyen/Workspace/dataset/twitter-trending/TT-classification/dataset/', trend_list, 'en')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['trend_hash'].value_counts()\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: text_preprocess.remove_hyperlink(x))\n",
    "\n",
    "df_trend = df[['text', 'trend_hash']]\n",
    "\n",
    "trend_label = text_features.trend_mapping(set(df_trend['trend_hash'].tolist()))\n",
    "\n",
    "df_trend['label'] = df_trend.apply(lambda row: trend_label[row.trend_hash], axis=1)"
   ]
  },
  {
   "source": [
    "## Latent Dirichlet Allocation Topic Modeling\n",
    "\n",
    "### Unigram feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, features = text_features.generate_term_freq(df['text'], 1)\n",
    "\n",
    "tf_uni_array = tf.toarray()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=22,\n",
    "                                    max_iter=10, \n",
    "                                    learning_method='online', \n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "print_top_word(lda, features, 10)"
   ]
  },
  {
   "source": [
    "### Bigram feature "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf, features = text_features.generate_term_freq(df['text'], 2)\n",
    "\n",
    "tf_bi_array = tf.toarray()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=22,\n",
    "                                    max_iter=10, \n",
    "                                    learning_method='online', \n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "\n",
    "lda.fit(tf)\n",
    "\n",
    "print_top_word(lda, features, 10)"
   ]
  },
  {
   "source": [
    "## K-Means clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_trend['label'].tolist()\n",
    "true_k = len(set(labels))\n",
    "\n",
    "tfidf, terms = text_features.generate_tfidf(df['text'])\n",
    "\n",
    "tfidf_array = tfidf.toarray()\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "            verbose=1)\n",
    "km.fit(tfidf)\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "                    % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s /' % terms[ind], end='')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}